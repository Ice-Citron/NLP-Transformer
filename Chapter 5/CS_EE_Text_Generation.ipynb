{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nlp-with-transformers/notebooks.git\n",
        "%cd notebooks\n",
        "#from install import *\n",
        "#install_requirements()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR5VZNHGofFa",
        "outputId": "592d4eb7-c032-4903-de25-65d9e1379486"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'notebooks' already exists and is not an empty directory.\n",
            "/content/notebooks/notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "!pip install transformers==4.41.2\n",
        "!pip install datasets==2.20.0\n",
        "\n",
        "!pip install pyarrow==16.0\n",
        "!pip install requests==2.32.3\n",
        "\n",
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0\n",
        "\n",
        "!pip install importlib-metadata\n",
        "\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pch7H0QoofDj",
        "outputId": "1aee25bb-79fb-47c0-add1-0c065b662175"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.41.2 in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.2) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.2) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.2) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.2) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.2) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.41.2) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.41.2) (2024.6.2)\n",
            "Requirement already satisfied: datasets==2.20.0 in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (3.15.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (16.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.20.0) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.20.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.20.0) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.16.0)\n",
            "Requirement already satisfied: pyarrow==16.0 in /usr/local/lib/python3.10/dist-packages (16.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==16.0) (1.25.2)\n",
            "Requirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.32.3) (2024.6.2)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision==0.18.0 in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: torchaudio==2.3.0 in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (9.4.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata) (3.19.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import *\n",
        "setup_chapter()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzCQIrmfowN3",
        "outputId": "1f3d3eea-563c-4dbf-b795-cee0e18455a6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU was detected! This notebook can be *very* slow without a GPU ðŸ¢\n",
            "Go to Runtime > Change runtime type and select a GPU hardware accelerator.\n",
            "Using transformers v4.41.2\n",
            "Using datasets v2.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "# Verifying packages installed are now up to date\n",
        "!pip show pyarrow requests transformers datasets torch torchaudio importlib-metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py4k68sSoe_s",
        "outputId": "f4b8ed0d-973a-4446-88a6-318921b9bc83"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: pyarrow\n",
            "Version: 16.0.0\n",
            "Summary: Python library for Apache Arrow\n",
            "Home-page: https://arrow.apache.org/\n",
            "Author: \n",
            "Author-email: \n",
            "License: Apache License, Version 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numpy\n",
            "Required-by: bigframes, cudf-cu12, datasets, db-dtypes, ibis-framework, pandas-gbq, tensorflow-datasets\n",
            "---\n",
            "Name: requests\n",
            "Version: 2.32.3\n",
            "Summary: Python HTTP for Humans.\n",
            "Home-page: https://requests.readthedocs.io\n",
            "Author: Kenneth Reitz\n",
            "Author-email: me@kennethreitz.org\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: certifi, charset-normalizer, idna, urllib3\n",
            "Required-by: bigframes, CacheControl, community, datasets, earthengine-api, fastai, folium, gcsfs, gdown, geocoder, google-api-core, google-cloud-bigquery, google-cloud-storage, google-colab, huggingface-hub, kaggle, kagglehub, moviepy, music21, pandas-datareader, panel, pooch, pymystem3, requests-oauthlib, spacy, Sphinx, tensorboard, tensorflow-datasets, torchtext, transformers, tweepy, weasel, yfinance\n",
            "---\n",
            "Name: transformers\n",
            "Version: 4.41.2\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: \n",
            "---\n",
            "Name: datasets\n",
            "Version: 2.20.0\n",
            "Summary: HuggingFace community-driven open-source library of datasets\n",
            "Home-page: https://github.com/huggingface/datasets\n",
            "Author: HuggingFace Inc.\n",
            "Author-email: thomas@huggingface.co\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyarrow-hotfix, pyyaml, requests, tqdm, xxhash\n",
            "Required-by: \n",
            "---\n",
            "Name: torch\n",
            "Version: 2.3.0+cu121\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
            "Required-by: accelerate, fastai, torchaudio, torchtext, torchvision\n",
            "---\n",
            "Name: torchaudio\n",
            "Version: 2.3.0+cu121\n",
            "Summary: An audio package for PyTorch\n",
            "Home-page: https://github.com/pytorch/audio\n",
            "Author: Soumith Chintala, David Pollack, Sean Naren, Peter Goldsborough, Moto Hira, Caroline Chen, Jeff Hwang, Zhaoheng Ni, Xiaohui Zhang\n",
            "Author-email: soumith@pytorch.org\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: torch\n",
            "Required-by: \n",
            "---\n",
            "Name: importlib_metadata\n",
            "Version: 7.2.0\n",
            "Summary: Read metadata from Python packages\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \"Jason R. Coombs\" <jaraco@jaraco.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: zipp\n",
            "Required-by: dask\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuMva5J6oeza",
        "outputId": "5975c6c8-5913-4686-d471-d609f78aa8d1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cat /proc/cpuinfo"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ju7NiHM2DfVk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIyAV0LzoUVV",
        "outputId": "8811a333-b965-441e-b24b-ab82c52fa3bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:2.3.0+cu121\n",
            "Transformers Version:4.41.2\n",
            "Datasets Version:2.20.0\n",
            "Tokenizers Version:0.19.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import datasets\n",
        "import tokenizers\n",
        "\n",
        "print(\"PyTorch Version:\" + torch.__version__)\n",
        "print(\"Transformers Version:\" + transformers.__version__)\n",
        "print(\"Datasets Version:\" + datasets.__version__)\n",
        "print(\"Tokenizers Version:\" + tokenizers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hide_output\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"gpt2-xl\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "QW63AOuZ2Bvb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Greedy Search Decoding\n",
        "\n",
        "Greedy search decoding is a simple and common method used in natural language processing (NLP), especially in the context of text generation with transformer models. It operates under a straightforward principle: at each step in generating text, it selects the next word that has the highest probability of occurrence given the previous words in the sequence. Hereâ€™s a breakdown of how it works:\n",
        "\n",
        "### Mechanism of Greedy Search Decoding\n",
        "\n",
        "1. **Initialization**: The process begins with an initial input, which can be a start token or a prompt provided by the user. The model uses this input to predict the probabilities of the next possible words.\n",
        "\n",
        "2. **Word Selection**: Out of the predicted probabilities for the next words, the word with the highest probability is chosen as the next word in the sequence.\n",
        "\n",
        "3. **Sequence Update**: This chosen word is then appended to the sequence.\n",
        "\n",
        "4. **Repetition**: The updated sequence (original input plus the new word) is fed back into the model. This process is repeated until a stop condition is metâ€”typically when a maximum sequence length is reached or a special end-of-sequence token is generated.\n",
        "\n",
        "5. **Output**: The final sequence of words generated through this method forms the completed text.\n",
        "\n",
        "### Advantages and Disadvantages\n",
        "\n",
        "**Advantages**:\n",
        "- **Speed**: Greedy search is computationally efficient because it only requires a single forward pass through the model to select the highest probability word at each step.\n",
        "- **Simplicity**: It is straightforward to implement and understand.\n",
        "\n",
        "**Disadvantages**:\n",
        "- **Lack of Diversity**: Since it always chooses the most likely word, greedy search can lead to repetitive and generic text. It often misses more interesting or nuanced combinations of words that might have a slightly lower probability but could contribute to a more coherent or creative overall piece.\n",
        "- **Risk of Getting Stuck**: Greedy search can sometimes get stuck in suboptimal loops or dead ends where the text becomes nonsensical or overly repetitive, as it does not reconsider its past choices.\n",
        "\n",
        "### Comparison to Other Decoding Methods\n",
        "\n",
        "In contrast to greedy search, other decoding methods like beam search or sampling-based approaches (e.g., top-k sampling, nucleus sampling) offer alternatives that balance between the likelihood of words and the diversity of the generated text. Beam search, for instance, keeps track of a number of hypotheses at each step (the \"beam width\"), and only the best hypotheses according to their cumulative probabilities are extended. This often results in better quality outputs compared to greedy search.\n",
        "\n",
        "Greedy search is often used when a fast, deterministic output is needed, but in cases where quality and diversity of text are more important, other methods are generally preferred."
      ],
      "metadata": {
        "id": "DfX7UfhP_yTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hide_output\n",
        "import pandas as pd\n",
        "\n",
        "input_txt = \"Transformers are the\"\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device) # sets up initial variables, input. By tokenising it.\n",
        "                                                                              # So that it can be passed to the transfoemr.\n",
        "iterations = []\n",
        "n_steps = 8\n",
        "choices_per_step = 5\n",
        "\n",
        "with torch.no_grad(): # inference, no need to automatically calculate gradient\n",
        "    for _ in range(n_steps):\n",
        "        iteration = dict() # creates empty dictionary\n",
        "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
        "        output = model(input_ids=input_ids)\n",
        "        # Select logits of the first batch and the last token and apply softmax\n",
        "        next_token_logits = output.logits[0, -1, :] # ':' selects all elements along this dimension\n",
        "        print(output.logits.size())\n",
        "        next_token_probs = torch.softmax(next_token_logits, dim=-1) # applies softmax to next_token_logits\n",
        "\n",
        "        # Sort tokens in descending order of probability\n",
        "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
        "\n",
        "        # Store tokens with highest probabilities\n",
        "        for choice_idx in range(choices_per_step):\n",
        "            token_id = sorted_ids[choice_idx]\n",
        "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
        "            token_choice = (\n",
        "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
        "            )\n",
        "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
        "        # Append predicted next token to input\n",
        "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1) # hence for the next iteration, the output will be used.\n",
        "        print(iteration)\n",
        "        iterations.append(iteration)\n",
        "\n",
        "pd.DataFrame(iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "60ZL_kmo2N9q",
        "outputId": "13c69f37-48fb-4249-81be-1380d2dfafae"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 50257])\n",
            "{'Input': 'Transformers are the', 'Choice 1': ' most (8.53%)', 'Choice 2': '\n",
            "only (4.96%)', 'Choice 3': ' best (4.65%)', 'Choice 4': ' Transformers (4.37%)',\n",
            "'Choice 5': ' ultimate (2.16%)'}\n",
            "torch.Size([1, 5, 50257])\n",
            "{'Input': 'Transformers are the most', 'Choice 1': ' popular (16.78%)', 'Choice\n",
            "2': ' powerful (5.37%)', 'Choice 3': ' common (4.96%)', 'Choice 4': ' famous\n",
            "(3.72%)', 'Choice 5': ' successful (3.20%)'}\n",
            "torch.Size([1, 6, 50257])\n",
            "{'Input': 'Transformers are the most popular', 'Choice 1': ' toy (10.63%)',\n",
            "'Choice 2': ' toys (7.23%)', 'Choice 3': ' Transformers (6.60%)', 'Choice 4': '\n",
            "of (5.46%)', 'Choice 5': ' and (3.76%)'}\n",
            "torch.Size([1, 7, 50257])\n",
            "{'Input': 'Transformers are the most popular toy', 'Choice 1': ' line (34.38%)',\n",
            "'Choice 2': ' in (18.20%)', 'Choice 3': ' of (11.71%)', 'Choice 4': ' brand\n",
            "(6.10%)', 'Choice 5': 'line (2.69%)'}\n",
            "torch.Size([1, 8, 50257])\n",
            "{'Input': 'Transformers are the most popular toy line', 'Choice 1': ' in\n",
            "(46.29%)', 'Choice 2': ' of (15.09%)', 'Choice 3': ', (4.94%)', 'Choice 4': ' on\n",
            "(4.40%)', 'Choice 5': ' ever (2.72%)'}\n",
            "torch.Size([1, 9, 50257])\n",
            "{'Input': 'Transformers are the most popular toy line in', 'Choice 1': ' the\n",
            "(65.99%)', 'Choice 2': ' history (12.42%)', 'Choice 3': ' America (6.91%)',\n",
            "'Choice 4': ' Japan (2.44%)', 'Choice 5': ' North (1.40%)'}\n",
            "torch.Size([1, 10, 50257])\n",
            "{'Input': 'Transformers are the most popular toy line in the', 'Choice 1': '\n",
            "world (69.27%)', 'Choice 2': ' United (4.55%)', 'Choice 3': ' history (4.29%)',\n",
            "'Choice 4': ' US (4.23%)', 'Choice 5': ' U (2.30%)'}\n",
            "torch.Size([1, 11, 50257])\n",
            "{'Input': 'Transformers are the most popular toy line in the world', 'Choice 1':\n",
            "', (39.73%)', 'Choice 2': '. (30.64%)', 'Choice 3': ' and (9.87%)', 'Choice 4':\n",
            "' with (2.32%)', 'Choice 5': ' today (1.74%)'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Input           Choice 1  \\\n",
              "0                               Transformers are the       most (8.53%)   \n",
              "1                          Transformers are the most   popular (16.78%)   \n",
              "2                  Transformers are the most popular       toy (10.63%)   \n",
              "3              Transformers are the most popular toy      line (34.38%)   \n",
              "4         Transformers are the most popular toy line        in (46.29%)   \n",
              "5      Transformers are the most popular toy line in       the (65.99%)   \n",
              "6  Transformers are the most popular toy line in the     world (69.27%)   \n",
              "7  Transformers are the most popular toy line in ...         , (39.73%)   \n",
              "\n",
              "            Choice 2               Choice 3               Choice 4  \\\n",
              "0       only (4.96%)           best (4.65%)   Transformers (4.37%)   \n",
              "1   powerful (5.37%)         common (4.96%)         famous (3.72%)   \n",
              "2       toys (7.23%)   Transformers (6.60%)             of (5.46%)   \n",
              "3        in (18.20%)            of (11.71%)          brand (6.10%)   \n",
              "4        of (15.09%)              , (4.94%)             on (4.40%)   \n",
              "5   history (12.42%)        America (6.91%)          Japan (2.44%)   \n",
              "6     United (4.55%)        history (4.29%)             US (4.23%)   \n",
              "7         . (30.64%)            and (9.87%)           with (2.32%)   \n",
              "\n",
              "              Choice 5  \n",
              "0     ultimate (2.16%)  \n",
              "1   successful (3.20%)  \n",
              "2          and (3.76%)  \n",
              "3         line (2.69%)  \n",
              "4         ever (2.72%)  \n",
              "5        North (1.40%)  \n",
              "6            U (2.30%)  \n",
              "7        today (1.74%)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be30a9ca-84a3-47b3-aec9-2836e7b3cbaa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Choice 1</th>\n",
              "      <th>Choice 2</th>\n",
              "      <th>Choice 3</th>\n",
              "      <th>Choice 4</th>\n",
              "      <th>Choice 5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Transformers are the</td>\n",
              "      <td>most (8.53%)</td>\n",
              "      <td>only (4.96%)</td>\n",
              "      <td>best (4.65%)</td>\n",
              "      <td>Transformers (4.37%)</td>\n",
              "      <td>ultimate (2.16%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Transformers are the most</td>\n",
              "      <td>popular (16.78%)</td>\n",
              "      <td>powerful (5.37%)</td>\n",
              "      <td>common (4.96%)</td>\n",
              "      <td>famous (3.72%)</td>\n",
              "      <td>successful (3.20%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Transformers are the most popular</td>\n",
              "      <td>toy (10.63%)</td>\n",
              "      <td>toys (7.23%)</td>\n",
              "      <td>Transformers (6.60%)</td>\n",
              "      <td>of (5.46%)</td>\n",
              "      <td>and (3.76%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Transformers are the most popular toy</td>\n",
              "      <td>line (34.38%)</td>\n",
              "      <td>in (18.20%)</td>\n",
              "      <td>of (11.71%)</td>\n",
              "      <td>brand (6.10%)</td>\n",
              "      <td>line (2.69%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Transformers are the most popular toy line</td>\n",
              "      <td>in (46.29%)</td>\n",
              "      <td>of (15.09%)</td>\n",
              "      <td>, (4.94%)</td>\n",
              "      <td>on (4.40%)</td>\n",
              "      <td>ever (2.72%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Transformers are the most popular toy line in</td>\n",
              "      <td>the (65.99%)</td>\n",
              "      <td>history (12.42%)</td>\n",
              "      <td>America (6.91%)</td>\n",
              "      <td>Japan (2.44%)</td>\n",
              "      <td>North (1.40%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Transformers are the most popular toy line in the</td>\n",
              "      <td>world (69.27%)</td>\n",
              "      <td>United (4.55%)</td>\n",
              "      <td>history (4.29%)</td>\n",
              "      <td>US (4.23%)</td>\n",
              "      <td>U (2.30%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Transformers are the most popular toy line in ...</td>\n",
              "      <td>, (39.73%)</td>\n",
              "      <td>. (30.64%)</td>\n",
              "      <td>and (9.87%)</td>\n",
              "      <td>with (2.32%)</td>\n",
              "      <td>today (1.74%)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be30a9ca-84a3-47b3-aec9-2836e7b3cbaa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-be30a9ca-84a3-47b3-aec9-2836e7b3cbaa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-be30a9ca-84a3-47b3-aec9-2836e7b3cbaa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7f74d6ee-6bf3-4747-875d-3f4b659395d9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7f74d6ee-6bf3-4747-875d-3f4b659395d9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7f74d6ee-6bf3-4747-875d-3f4b659395d9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Transformers are the most\",\n          \"Transformers are the most popular toy line in\",\n          \"Transformers are the\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \" popular (16.78%)\",\n          \" the (65.99%)\",\n          \" most (8.53%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \" powerful (5.37%)\",\n          \" history (12.42%)\",\n          \" only (4.96%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \" common (4.96%)\",\n          \" America (6.91%)\",\n          \" best (4.65%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \" famous (3.72%)\",\n          \" Japan (2.44%)\",\n          \" Transformers (4.37%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \" successful (3.20%)\",\n          \" North (1.40%)\",\n          \" ultimate (2.16%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable `output` in this context contains the predictions from the transformer model at each step of the text generation. These predictions are generally in the form of logits, which are the raw, unnormalized outputs of the last layer of the neural network.\n",
        "\n",
        "### What \"output\" Contains\n",
        "\n",
        "1. **Logits**: Each element of the logits represents the raw score for each possible token in the model's vocabulary. The higher the score, the higher the probability of the token being the appropriate next word in the sequence, after applying a softmax function.\n",
        "\n",
        "2. **Shape of the Output**: The shape of `output.logits` is typically `[batch_size, sequence_length, vocab_size]`.\n",
        "   - **`batch_size`**: Number of sequences processed together. In your code, since you are processing one input sequence at a time, `batch_size` is 1.\n",
        "   - **`sequence_length`**: Length of the input text sequence being processed. This grows with each iteration since you are appending a new token to `input_ids` after each step.\n",
        "   - **`vocab_size`**: The total number of tokens in the model's vocabulary. This determines the last dimension's size, representing the score for each possible token.\n",
        "\n",
        "### Selection `[0, -1, :]`\n",
        "\n",
        "Hereâ€™s why each component of this slicing is used:\n",
        "\n",
        "- **`[0]`**: Since the `batch_size` is 1, this index is used to select the output corresponding to the first and only sequence in the batch. Using batch size of 1 is common in generation tasks where sequences are generated one at a time.\n",
        "\n",
        "- **`-1`**: This selects the output for the last token in the current sequence. In the context of sequential generation, the last token is where the next prediction bases upon. You continue the sequence from where it last left off, hence focusing on the output for the last token processed.\n",
        "\n",
        "- **`:`**: This selects all elements across the last dimension, which correspond to the logits of each token in the vocabulary.\n",
        "\n",
        "### Practical Implication\n",
        "\n",
        "Using `[0, -1, :]` allows the model to focus on just the necessary part of the outputâ€”specifically, the logits for the next word prediction based on the last word of the sequence. This is efficient and avoids unnecessary computations on earlier parts of the sequence that are already established in earlier steps of generation. Each iteration then builds on the previous by extending the sequence one token at a time, and this slicing ensures that only the most recent token's output is used to determine the next step in the sequence."
      ],
      "metadata": {
        "id": "IW1eThj1C-BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False) # do_sample=False is greedy decoding. The most probable next token is always chosen.\n",
        "                                                                            # do_sample=True will be explained subsequently\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUt9EMdW2OZB",
        "outputId": "dd8b15ac-1fe1-4962-8adc-6db9c2de6a10"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers are the most popular toy line in the world,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
        "a herd of unicorns living in a remote, previously unexplored \\\n",
        "valley, in the Andes Mountains. Even more surprising to the \\\n",
        "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
        "\"\"\"\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
        "print(tokenizer.decode(output_greedy[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg9A5vpJ2Pnj",
        "outputId": "a6c94c79-7d6c-4638-c371-c69d4901dbec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a shocking finding, scientist discovered a herd of unicorns living in a\n",
            "remote, previously unexplored valley, in the Andes Mountains. Even more\n",
            "surprising to the researchers was the fact that the unicorns spoke perfect\n",
            "English.\n",
            "\n",
            "\n",
            "The researchers, from the University of California, Davis, and the University of\n",
            "Colorado, Boulder, were conducting a study on the Andean cloud forest, which is\n",
            "home to the rare species of cloud forest trees.\n",
            "\n",
            "\n",
            "The researchers were surprised to find that the unicorns were able to\n",
            "communicate with each other, and even with humans.\n",
            "\n",
            "\n",
            "The researchers were surprised to find that the unicorns were able\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam Search Decoding"
      ],
      "metadata": {
        "id": "CoOzwSZMIxKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# far as i can see. this function converts the raw output scores (logits) from a model into log probabilities for specific tokens identified by the labels.\n",
        "\n",
        "def log_probs_from_logits(logits, labels): # logits: raw output scores from model, typically before applying softmax\n",
        "                                           # labels: indices of tokens (usually correct or chosen token during training or evaluation)\n",
        "    logp = F.log_softmax(logits, dim=-1)  # converts logits into log probabilities. Softmax normalises logits to probabilities. Then log is taken to convert these\n",
        "                                          # probabilities into log probabilities.\n",
        "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
        "    return logp_label"
      ],
      "metadata": {
        "id": "TzO87aWl_gbf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Let's delve deeper into the specific operation performed by `torch.gather` and how it is used to extract log probabilities for specified tokens in a sequence, with a detailed example to illustrate the process.\n",
        "\n",
        "### Understanding `torch.gather`\n",
        "\n",
        "`torch.gather` is a PyTorch function used to gather values from a tensor along a specified dimension based on index values provided in another tensor. Hereâ€™s the general usage:\n",
        "\n",
        "```python\n",
        "torch.gather(input, dim, index)\n",
        "```\n",
        "- **input**: The source tensor from which to gather values.\n",
        "- **dim**: The dimension along which to index.\n",
        "- **index**: The tensor containing the indices of elements to gather.\n",
        "\n",
        "### The Specific Case: `logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)`\n",
        "\n",
        "This line of code is involved in selecting specific log probabilities from a batch of sequences. Let's break it down:\n",
        "\n",
        "1. **`logp = F.log_softmax(logits, dim=-1)`**:\n",
        "   - This computes the logarithm of softmax probabilities along the last dimension (dim=-1) of the logits tensor. Assume `logits` has a shape of `[batch_size, sequence_length, vocab_size]`, then `logp` will have the same shape.\n",
        "\n",
        "2. **`labels.unsqueeze(2)`**:\n",
        "   - `labels` typically has a shape of `[batch_size, sequence_length]`, where each entry is the index of the true or next token in the sequence.\n",
        "   - `unsqueeze(2)` adds a third dimension, changing its shape to `[batch_size, sequence_length, 1]`. This is necessary to make it compatible for gathering along the third dimension (vocab_size) of `logp`.\n",
        "\n",
        "3. **`torch.gather(logp, 2, labels.unsqueeze(2))`**:\n",
        "   - This gathers values from `logp` based on indices specified in `labels`. Since `labels` now has an extra dimension, it can directly index into the vocab_size dimension of `logp`.\n",
        "   - The resulting tensor has the same shape as `labels.unsqueeze(2)`, which is `[batch_size, sequence_length, 1]`.\n",
        "\n",
        "4. **`.squeeze(-1)`**:\n",
        "   - This removes the last dimension (now redundant because it's of size 1), resulting in a shape of `[batch_size, sequence_length]`. Each element in this tensor is the log probability of the respective token in `labels`.\n",
        "\n",
        "### Example\n",
        "\n",
        "Assume:\n",
        "- `logits` tensor representing logits for a batch of 1 (batch_size=1), a sequence of 3 tokens (sequence_length=3), and a vocabulary size of 5 (vocab_size=5).\n",
        "- Each token can be any of the 5 vocabulary items.\n",
        "\n",
        "Python code example:\n",
        "```python\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example logits tensor (batch_size=1, sequence_length=3, vocab_size=5)\n",
        "logits = torch.tensor([[[1.0, 2.0, 3.0, 4.0, 5.0],\n",
        "                        [1.5, 2.5, 3.5, 4.5, 5.5],\n",
        "                        [2.0, 3.0, 4.0, 5.0, 6.0]]])\n",
        "\n",
        "# Convert logits to log probabilities\n",
        "logp = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "# Example labels (indices of actual tokens in the sequence)\n",
        "labels = torch.tensor([[0, 2, 4]])  # Corresponds to token indices 0, 2, and 4 for each step\n",
        "\n",
        "# Gather log probabilities for each label\n",
        "logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
        "\n",
        "print(\"Log probabilities for selected labels:\", logp_label)\n",
        "```\n",
        "\n",
        "Output explanation:\n",
        "- This script calculates the log probabilities for specific tokens in each position of the sequence according to `labels`. The output will show these probabilities, demonstrating how `torch.gather` efficiently extracts this data from a 3D tensor.\n",
        "\n",
        "This approach is powerful for tasks like computing loss during training, where you need to reference the probability assigned by the model to the actual token that appears at each sequence position."
      ],
      "metadata": {
        "id": "5ax84nwcNNI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, your understanding is correct. `torch.gather` fundamentally gathers elements from a tensor along a specified dimension, based on indices provided in another tensor. Here's a more detailed explanation of how it operates:\n",
        "\n",
        "### Basic Functionality of `torch.gather`\n",
        "The function `torch.gather` is used to create a new tensor by selecting specific elements from the input tensor. The selection is governed by indices specified in an index tensor, and it operates along a specified dimension.\n",
        "\n",
        "### Parameters\n",
        "- **input (Tensor)**: The source tensor from which elements will be gathered.\n",
        "- **dim (int)**: The dimension along which to index. This dimension will be accessed in the input tensor to select elements.\n",
        "- **index (LongTensor)**: The indices of elements to gather. This tensor must have the same shape as the input tensor, but along the specified dimension, each value must be a valid index in that dimension of the input tensor.\n",
        "\n",
        "### How It Works\n",
        "1. **Dimension Selection**: The function looks at the specified dimension (`dim`) in the input tensor.\n",
        "2. **Indexing**: For each index value in the `index` tensor, `torch.gather` picks the corresponding element from the `input` tensor along the chosen dimension.\n",
        "3. **Tensor Construction**: The output tensor is constructed using the gathered elements and retains the shape of the `index` tensor.\n",
        "\n",
        "### Example to Illustrate\n",
        "\n",
        "Let's visualize this with a simple example. Assume you have a 2D tensor and you want to select elements from each row:\n",
        "\n",
        "```plaintext\n",
        "Tensor A:\n",
        "[\n",
        " [a, b, c],\n",
        " [d, e, f],\n",
        " [g, h, i]\n",
        "]\n",
        "```\n",
        "\n",
        "If you want to select an element from each row using specific column indices, you might specify an index tensor like this:\n",
        "\n",
        "```plaintext\n",
        "Index Tensor:\n",
        "[\n",
        " [0],\n",
        " [2],\n",
        " [1]\n",
        "]\n",
        "```\n",
        "\n",
        "Using `torch.gather` with `dim=1` (selecting along columns within each row), the output will be:\n",
        "\n",
        "```plaintext\n",
        "Output Tensor:\n",
        "[\n",
        " [a],\n",
        " [f],\n",
        " [h]\n",
        "]\n",
        "```\n",
        "\n",
        "Here's how the selection is made:\n",
        "- From the first row `[a, b, c]`, it selects `a` (column index 0).\n",
        "- From the second row `[d, e, f]`, it selects `f` (column index 2).\n",
        "- From the third row `[g, h, i]`, it selects `h` (column index 1).\n",
        "\n",
        "### Practical Uses\n",
        "This functionality is particularly useful in machine learning tasks where you need to extract specific predictions or outputs corresponding to certain indices. For example, in classification tasks, if you have the logits for multiple classes and you know the actual classes (as indices), you can use `torch.gather` to pick out the logits for the actual classes to compute the loss using a log-softmax operation.\n",
        "\n",
        "`torch.gather` is a versatile tool in tensor manipulation, allowing for complex operations that require selective indexing from higher-dimensional data based on dynamically generated indices."
      ],
      "metadata": {
        "id": "DOadKxECNfmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_logprob(model, labels, input_len=0):\n",
        "    with torch.no_grad():\n",
        "        output = model(labels)\n",
        "        log_probs = log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:])\n",
        "        seq_log_prob = torch.sum(log_probs[:, input_len:]) # summing in log values is basically same as multiplying\n",
        "                    # i think i get it now. Basically log_probs_from_logits will output all of the probabilities of all the labels in \"labels\" tensor\n",
        "                    # which is then summed (or multipled because its logp). And hence the probability of this beam as a whole is calculated!\n",
        "    return seq_log_prob.cpu().numpy()"
      ],
      "metadata": {
        "id": "2i0e87ulIzu5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))    # output_greedy here is previously defined from generate()\n",
        "print(tokenizer.decode(output_greedy[0]))\n",
        "print(f\"\\nlog-prob: {logp:.2f}\") # log probability value calculated for the beam generated by greedy decoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJZKp0vnIzth",
        "outputId": "a34e1fdc-fd32-4f80-9dda-e909aaff289c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a shocking finding, scientist discovered a herd of unicorns living in a\n",
            "remote, previously unexplored valley, in the Andes Mountains. Even more\n",
            "surprising to the researchers was the fact that the unicorns spoke perfect\n",
            "English.\n",
            "\n",
            "\n",
            "The researchers, from the University of California, Davis, and the University of\n",
            "Colorado, Boulder, were conducting a study on the Andean cloud forest, which is\n",
            "home to the rare species of cloud forest trees.\n",
            "\n",
            "\n",
            "The researchers were surprised to find that the unicorns were able to\n",
            "communicate with each other, and even with humans.\n",
            "\n",
            "\n",
            "The researchers were surprised to find that the unicorns were able\n",
            "\n",
            "log-prob: -87.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down what the slicing `output.logits[:, -1, :]` and the indexing `labels[:, 1:]` in your example are selecting and why they are used in the function `log_probs_from_logits`. I'll illustrate this with a clear example to help explain the concept.\n",
        "\n",
        "### Understanding the Slicing and Indexing\n",
        "\n",
        "1. **`output.logits[:, -1, :]`**:\n",
        "   - **`output.logits`** typically has a shape of `[batch_size, sequence_length, vocab_size]`. This tensor contains the logits for every token in the sequence for each example in the batch.\n",
        "   - **`[:, -1, :]`** slices the tensor to select the logits for the **last token** in each sequence for all examples in the batch. The `-1` in the second dimension specifies the last element in the sequence, which is often the most recent token predicted by the model during sequence generation or the final classification token in tasks like sequence classification.\n",
        "\n",
        "2. **`labels[:, 1:]`**:\n",
        "   - **`labels`** is a tensor with dimensions `[batch_size, sequence_length]`, containing the indices of the actual tokens (correct labels) for each position in the sequence for each example.\n",
        "   - **`[:, 1:]`** adjusts the tensor to exclude the first token's label in each sequence. This adjustment is typically made because the first token might be a special start token (like `[CLS]`, `[START]`, etc.) that is not predicted by the model but instead used as a starting input.\n",
        "\n",
        "### Example to Illustrate\n",
        "\n",
        "Let's consider a simple example with a batch size of 1 for simplicity. Suppose we have a vocabulary with five tokens (0 to 4), and a model predicts logits for a sequence of three tokens.\n",
        "\n",
        "**Logits Tensor (`output.logits`)**:\n",
        "```plaintext\n",
        "[\n",
        "  [[-0.1, -1.5,  0.3,  2.0, -0.5],  # Logits for token 1\n",
        "   [ 0.2,  0.0, -0.2, -1.2,  1.8],  # Logits for token 2\n",
        "   [ 1.0, -1.0,  0.5,  0.2, -0.4]]  # Logits for token 3\n",
        "]\n",
        "```\n",
        "\n",
        "**Labels Tensor (`labels`)**:\n",
        "```plaintext\n",
        "[\n",
        "  [0, 2, 4]  # Actual correct labels for the sequence\n",
        "]\n",
        "```\n",
        "\n",
        "#### Operation\n",
        "\n",
        "- **Selecting Logits**: `output.logits[:, -1, :]` results in the logits for the last token in the sequence, which are `[1.0, -1.0, 0.5, 0.2, -0.4]`.\n",
        "\n",
        "- **Adjusting Labels**: `labels[:, 1:]` results in `[2, 4]`. This skips the label for the first token, focusing on the tokens that follow.\n",
        "\n",
        "Now, if we want to fetch the log probabilities for these selected labels (token indices `2` and `4` for the last two tokens in the sequence), we would:\n",
        "\n",
        "1. Apply softmax to the logits to convert them into probabilities.\n",
        "2. Take the logarithm of these probabilities to obtain log probabilities.\n",
        "3. Use `torch.gather` to select the log probabilities for the indices `[2, 4]` from the last two tokens.\n",
        "\n",
        "### Practical Use\n",
        "\n",
        "This process is critical in tasks like calculating the loss during training, where you need the model's prediction probabilities for the actual correct tokens to compute something like cross-entropy loss. It efficiently aligns model outputs (logits) with the targets (labels), focusing only on the relevant parts of the output for loss computation."
      ],
      "metadata": {
        "id": "KtHKc7ONTTVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now comparing with a sequence that's generated by beam search\n",
        "        # to activate beam search, neede to specify 'num_beams' parameter\n",
        "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n",
        "                             do_sample=False)\n",
        "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
        "print(tokenizer.decode(output_beam[0]))\n",
        "print(f\"\\nlog-prob: {logp:.2f}\") # can see that log-prob of beam search is much higher than greedy encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmYVZD0QIzsF",
        "outputId": "eb3f0cf4-f541-4e23-8eda-a9b7d234db79"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a shocking finding, scientist discovered a herd of unicorns living in a\n",
            "remote, previously unexplored valley, in the Andes Mountains. Even more\n",
            "surprising to the researchers was the fact that the unicorns spoke perfect\n",
            "English.\n",
            "\n",
            "\n",
            "The discovery of the unicorns was made by a team of scientists from the\n",
            "University of California, Santa Cruz, and the National Geographic Society.\n",
            "\n",
            "\n",
            "The scientists were conducting a study of the Andes Mountains when they\n",
            "discovered a herd of unicorns living in a remote, previously unexplored valley,\n",
            "in the Andes Mountains. Even more surprising to the researchers was the fact\n",
            "that the unicorns spoke perfect English\n",
            "\n",
            "log-prob: -55.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# However, this beam search still suffers from text repetitiveness. We now address this by imposing an n-gram penalty, with\n",
        "# no_repeat_ngram_size parameter, that tracks which n-grams have been seen and sets the next token probability to 0 if it would produce a previously\n",
        "# seen n-gram.\n",
        "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n",
        "                             do_sample=False, no_repeat_ngram_size=2)\n",
        "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
        "print(tokenizer.decode(output_beam[0]))\n",
        "print(f\"\\nlog-prob: {logp:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8VTt078Izqr",
        "outputId": "c04f74a8-60f7-40b9-e36c-bb570470103d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a shocking finding, scientist discovered a herd of unicorns living in a\n",
            "remote, previously unexplored valley, in the Andes Mountains. Even more\n",
            "surprising to the researchers was the fact that the unicorns spoke perfect\n",
            "English.\n",
            "\n",
            "\n",
            "The discovery was made by a team of scientists from the University of\n",
            "California, Santa Cruz, and the National Geographic Society.\n",
            "\n",
            "According to a press release, the scientists were conducting a survey of the\n",
            "area when they came across the herd. They were surprised to find that they were\n",
            "able to converse with the animals in English, even though they had never seen a\n",
            "unicorn in person before. The researchers were\n",
            "\n",
            "log-prob: -93.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-53F0OxdIzpI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zN6HFk1_Izn7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wz0v8FRLIzmd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g7cqPFESIzjq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZtdgMIVLIzge"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NDp4YmcfIzea"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}